\section{Iteration 1}
\label{sec:Iteration-1-Agent-Environment}

The first iteration of the implementation focuses on having a minimum viable product \(MVP\). The MVP
consists of all MUST (M) requirements, which can be used as a baseline for next iterations.
Look chapter \ref{sec:functional-requirements} for a complete list of the MUST have requiremetns.
The MVP consists of a custom agent, with a custom training loop, that plays against a poke-env random agent,
in a generation 9, single battle pokemon showdown environment, where held items are available. 

\subsection{Poke-env Library}
The backbone of the first iteration of implementation is the Poke-env library. 
To simulate pokemon battles, we wrap our agent in a custom player class which inherits from the
Poke-env player class. This allowed us to model battles in any pokemon showdown 
provided battle format, such as Single or Double battles and retrieve relevant game information.
The agent was embedded into the environment, and could interacted through asynchronouns function
to reflect the turn-based format of pokemon battles.


\subsection{Deep Q-Learning Agent}
The agent is implemented using a Deep Q-Network (DQN) model. It consists of a Q-function
approximator via a naural network built using the Pytorch Library. Some key components 
are the state representation, action space, and exploration and learning process.
The state representation is a simple embedding consisting of normalized Hit Point (HP) values, 
for the active pokemon and opponent. Action space is a flattend list of all available actions, such
as moves, switching pokemon or using items. The exploration strategy is an epsilon-greedy strategy
with a decaying epsilon value, which is used to balance exploration and exploitation.
The learning process is stored in a replay buffer and used to train the model in batches, with 
periodic updates to the target network.
While this agent is functional, it had some limitations to its format and performance. The agent
uses a custom training loop that only accounts for a single battle format environment, and was 
battling using random pokemon teams each battle, to create a more knowledgeable agent. 
This created a more difficult environment for the agent to learn in and resulted in some 
edge cases, that could not be handled by the custom agent or opponent.
The agents learning parameters were like so:
\begin{itemize}
    \item \textbf{Batch Size:} 64
    \item \textbf{Replay Buffer Size:} 100000
    \item \textbf{Learning Rate:} 0.001 
    \item \textbf{Epsilon:} 1.0
    \item \textbf{Epsilon Decay:} 0.995
    \item \textbf{Epsilon Min:} 0.05
    \item \textbf{Tau} 0.005
    \item \textbf{Gamma:} 0.99
\end{itemize}



