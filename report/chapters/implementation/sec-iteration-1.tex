\section{Iteration 1}
\label{sec:Iteration-1-Agent-Environment}

The first iteration of the implementation focuses on having a minimum viable product \(MVP\). The MVP
consists of all MUST (M) requirements, which can be used as a baseline for next iterations.
Look chapter \ref{sec:functional-requirements} for a complete list of the MUST have requiremetns.
The MVP consists of a custom agent, with a custom training loop, that plays against a poke-env random agent,
in a generation 9, single battle pokemon showdown environment, where held items are available. 

\subsection{Poke-env Library}
The backbone of the first iteration of implementation is the Poke-env library. 
To simulate pokemon battles, we wrap our agent in a custom player class which inherits from the
Poke-env player class. This allowed us to model battles in any pokemon showdown 
provided battle format, such as Single or Double battles and retrieve relevant game information.
The agent was embedded into the environment, and could interacted through asynchronouns function
to reflect the turn-based format of pokemon battles.


\subsection{Deep Q-Learning Agent}
The agent is implemented using a Deep Q-Network (DQN) model. This approach leverages
a nerual network to approximate the Q-values, enabling the agent to learn an 
effective policy directly from the environments state representation. 
The Q-network was constructed using Pytorch and consists of a simple feedforward
architecture with two hidden layers. 
\begin{lstlisting}[basicstyle=\fontsize{10}{10}\selectfont\ttfamily,language=Python,caption={The defined action space.},label=lst:action-space-def,breaklines]
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = (nn.Linear(state_size, 64)) #Input layer -> Hidden layer
        self.fc2 = (nn.Linear(64, 64)) #Hidden layer -> Hidden layer
        self.fc3 = (nn.Linear(64, action_size)) #Hidden layer -> Output layer

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x 
\end{lstlisting}
This structure includes:
\begin{itemize}
    \item An input layer, that processes the state representation.
    \item Two hidden layers, each with 64 neurons and ReLu (rectified linear unit function) activations.
    \item An output layer, that produces Q-values for each possible action.
\end{itemize}

\subsubsection{State Representation and Action Space}
The state representation is a simple embedding consisting of the normalized Hit Point (HP) values
for the active pokemon and opponent. The action space is a flattend list that
include all available actions in the current state, such as moves, switching pokemon
or using items. These 2 features are simplified yet informative and generalized 
enough to be used in the two core formats of the pokemon game.

\subsubsection{Exploration, Learning Process and Training configuration}
To manage the exploration-exploitation trade-off, an epsilon-greedy ploicy is employed.
The epsilon value stats at 1.0, allowing the agent to explore in the early stages of training,
where after it decays after each episode with a factor of 0.995 with a lower bound of 0.05.
This ensures that the agent gradually shifts from exploration early in training
and focuses more on exploitation later on.
A replay buffer is used to store the agent's transitions (state, action, reward, next-state, done), 
and are sampled in batches for training, which can help stabilize the learning process 
by reducing correlations between sequential experiences. Furthermore, a target 
network is periodically updated to improve learning stability.
The agents was trained using the following hyperparameters:
\begin{itemize}
    \item \textbf{Batch Size:} 64
    \item \textbf{Replay Buffer Size:} 100000
    \item \textbf{Learning Rate:} 0.001 
    \item \textbf{Epsilon:} 1.0
    \item \textbf{Epsilon Decay:} 0.995
    \item \textbf{Epsilon Min:} 0.05
    \item \textbf{Tau:} 0.005
    \item \textbf{Gamma:} 0.99
\end{itemize}

\subsubsection{Limitations and Challenges}
while the first iteration of the implementation was successful in creating a custom agent
and training loop, it was faced with soem limitations and challenges.
\begin{itemize}
    \item The training loop was constrained to only single battle formats, with
    randomly generated teams, for both agents, for each episode. This was done to create a
    more generalzed agent, but introduced a high variance in the training process and came with more challenges.
    \item In some cases, the agent would be unable to finnish the training process, due to the 
    opponent agent not able to finish the battle. It would get stuck in a loop, trying to switch when it
    wasnt a valid action.
\end{itemize}