\section{Iteration 1: Agent and a Pokemon environment} 
\label{sec:Iteration-1-Agent-Environment}

The first iteration of the implementation focused on developing a foundational version of the 
agent that satisfies all MUST (M) requirements identified in Chapter 2.1. These essential requirements 
were prioritized to ensure that the agent could operate effectively within a structured Pokemon environment, 
forming a solid basis for further development and refinement in subsequent iterations.

In this initial version, a custom agent was implemented along with a dedicated training loop. The agent was 
designed to interact with Pokemon Showdown through the poke-env library, enabling participation in generation 9 single battles, 
including the use of held items and the generation specific mechanic Terastilization. To evaluate the agent's 
performance and support training, it was set up to play against a random opponent, also implemented using poke-env. 



\subsection{Poke-env Library}
The backbone of the first iteration of implementation is the Poke-env library. 
Poke-env is a Python library that provides a framework for creating and simulating Pokemon
battles in a variety of formats, including single and double battles and supports the latest
generation of Pokemon games, such as generation 9. Poke-env also provides a set of tools
for interacting with the Pokemon Showdown API, allowing for easier integration with the formats
and to not overload the online version of Pokemon Showdown with too many requests.

To simulate Pokemon battles, we wrap our agent in a custom player class which inherits from the
Poke-env player class. This allowed us to model the agent for any Pokemon Showdown 
provided battle format, such as Single, Double battles or VGC regulated battles and 
retrieve relevant game information. The agent was embedded into the environment, and could 
interact through an asynchronous function to reflect the turn-based format of Pokemon battles.



\subsection{Deep Q-Network Agent}
The agent is implemented using a Deep Q-Network (DQN) model. This approach leverages
a neural network to approximate the Q-values, enabling the agent to learn an 
effective policy directly from the environment's state representation. 
The Q-network was constructed using PyTorch and consists of a simple feedforward
architecture with two hidden layers. 
\begin{lstlisting}[basicstyle=\fontsize{10}{10}\selectfont\ttfamily,language=Python,caption={The defined action space.},label=lst:action-space-def,breaklines]
class DQN(nn.Module):
    def __init__(self, state_size, action_size):
        super(DQN, self).__init__()
        self.fc1 = (nn.Linear(state_size, 64)) #Input layer -> Hidden layer
        self.fc2 = (nn.Linear(64, 64)) #Hidden layer -> Hidden layer
        self.fc3 = (nn.Linear(64, action_size)) #Hidden layer -> Output layer

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x 
\end{lstlisting}
This structure includes:
\begin{itemize}
    \item An input layer, that processes the state representation.
    \item Two hidden layers, each with 64 neurons and ReLu (rectified linear unit function) activations.
    \item An output layer, that produces Q-values for each possible action.
\end{itemize}

\subsubsection{State Representation and Action Space}
The state representation is a simple embedding consisting of the normalized Hit Point (HP) values
for the active Pokemon and opponent. The action space is a flattened list that
includes all available actions in the current state, such as moves or switching Pokemon. 
These 2 features are simple yet informative and generalized 
enough to be used in the two core formats of the Pokemon game.

\subsubsection{Exploration, Learning Process and Training configuration}
To manage the exploration-exploitation trade-off, an epsilon-greedy policy is employed.
The epsilon value starts at 1.0, allowing the agent to explore in the early stages of training,
where it decays after each episode with a factor of 0.995 with a lower bound of 0.05.
This ensures that the agent gradually shifts from exploration early in training
and focuses more on exploitation later on.
A replay buffer is used to store the agent's transitions (state, action, reward, next-state, done), 
and are sampled in batches for training, which can help stabilize the learning process 
by reducing correlations between sequential experiences. Furthermore, a target 
network is periodically updated to improve learning stability.
The agent was trained using the following hyperparameters:
\begin{itemize}
    \item \textbf{Batch Size:} 64
    \item \textbf{Replay Buffer Size:} 100000
    \item \textbf{Learning Rate:} 0.001 
    \item \textbf{Epsilon:} 1.0
    \item \textbf{Epsilon Decay:} 0.995
    \item \textbf{Epsilon Min:} 0.05
    \item \textbf{Tau:} 0.005
    \item \textbf{Gamma:} 0.99
\end{itemize}

\subsubsection{Limitations and Challenges}
While the first iteration of the implementation was successful in creating a custom agent
and training loop, it was faced with some limitations and challenges.
\begin{itemize}
    \item The training loop was constrained to only single battle formats, with
    randomly generated teams, for both agents, for each episode. This was done to create a
    more generalzed agent, but introduced too high variance in the training process and came with more challenges.
    \item In some cases, the agent would be unable to finish the training process, due to the 
    opponent agent not able to finish the battle. It would get stuck in a loop, trying to switch when it
    wasn't a valid action.
\end{itemize}