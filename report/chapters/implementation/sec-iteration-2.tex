\section{Iteration 2: Agent Improvements}
\label{sec:Iteration-2-Agent-Environment}
The second iteration of implementation focused on restructuring the agent for improved 
modularity, better representation of game states and scaleability. This version of the agent still
uses the Poke-env library, but the agent is now training in a double battle environment. The
agent is now capable of handling multiple Pokemon and can make decisions based on the state of 
both opponent pokemon and allied pokemon. The agent also uses PyTorches own replay buffer for
training, which is a significant improvement over the previous custom version.

\subsection{Agent}
Our agent is now capable of accepting pre-made teams so long a team alligns with the required battle
format. The agents hyperparameters are now also much more modular, allowing for easy tuning and
we have moved the training loop inside the agent class, so that the asynchronous methods only
sends and accept challenges from the agent in the environment. The state size is now also 
calculated based on the number of pokemon in the team, and the number of pokemon in the opponents 
and their type. This means we have a lot more options for the agent to choose from, and learn 
the pokemons types. This also means that the Q-function approximator is now a lot more complex,
because the agent is now capable of learning from a much larger state space. 


\subsection{Reward Function}
A key component of any reinforcement learning agent is its reward function, 
which provides the feedback necessary for learning and adaptation. 
In this second iteration, the reward function was redesigned to provide a more 
informative and consistent signal to the agent, addressing several limitations identified in the first iteration.

In the previous version, the reward function was relatively simple: it assigned a reward based solely on 
whether the agent won or lost the battle. While this approach allowed the agent to learn a basic policy, 
it produced a flat and uninformative learning curve, as the agent received very similar rewards regardless 
of the battle dynamics. Consequently, it was difficult to measure nuanced improvements in the agent's performance over time.

To address these issues, the updated reward function now incorporates more detailed information about the 
state of the battle. Specifically, it takes into account the HP (Hit Points) of all Pokémon on the field 
and the remaining HP at the end of the battle. By doing so, the agent receives incremental feedback that 
reflects its performance throughout the match, rather than only the final outcome. This continuous feedback 
helps the agent understand the impact of its decisions, such as damaging opposing Pokémon, preserving its own 
team's health, or successfully fainting opponents.

\subsection{Replay Buffer} % Explain 
An essential component of the training pipeline in this iteration is the replay buffer, which plays 
a role in stabilizing the learning process and improving sample efficiency. The replay buffer 
comprises two main parts. First, it is responsible for storing observed experience tuples during training. 
These tuples capture important information from each interaction with the environment, consisting of:

\begin{itemize}
    \item \textbf{State:} The current representation of the environment at the time of action.
    \item \textbf{Action:} The specific action chosen by the agent in that state.
    \item \textbf{Reward:} The feedback received from the environment after taking the action.
    \item \textbf{Next State:} The state of the environment after the action has been taken.
    \item \textbf{Done:} A boolean indicating whether the episode has ended.
\end{itemize}

Second, the replay buffer is designed to sample small batches of these experience tuples randomly, 
rather than in the order they were encountered. This random sampling breaks the correlation between 
sequential experiences and reduces the risk of overfitting to recent episodes. It allows the agent 
to learn from a more diverse set of experiences and helps approximate the true distribution of 
the environment's dynamics. 
