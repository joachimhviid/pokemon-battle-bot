\section{Machine Learning}
\label{sec:Machine Learning}

Machine learning is a subset of artificial intelligence
that focuses on the development of a computer program that learns from data.
In machine learning there are three approaches: supervised, unsupervised, and reinforcement learning.
Each of these approaches has its own strengths and weaknesses, but all of them need a machine learning algorithm.

A machine learning algorithm is a set of rules or mathematical
procedures that the system follows to learn from a dataset
and make predictions or decisions based on the data.

After applying the algorithm to a dataset, you will have what is called a Machine learning Model.
A model is a specific instance of a trained system that has learned from the data and can make predictions or classifications.
\cite{ML-Models}
\textbf{Analogy:} A machine learning model is like a fully cooked meal.
The recipe (algorithm) was used to prepare it, but now it is a ready-to-use product.

\subsection{Supervised Learning}
The supervised learning approach is the more common approach of the 3 machine learning approaches.
This is due to its ability to predict a wide range of problems accurately,
however its effectiveness is dependent on the quality of the training data.
Supervised learning uses labeled datasets to train a model,
which means that from the input data we expect the correct output data as well.
\cite{GoogleCloud-SL}

Some examples of supervised learning are spam email classifiers,
so whether or not an email is spam, and weather prediction models.

\subsection{Unsupervised Learning}
Unsupervised learning is a type of machine learning that learns from data without human supervision. Unlike supervised learning, 
where models are trained on labeled datasets with known outcomes, unsupervised machine learning models are given unlabeled data 
and are allowed to discover patterns and relationships within the data on their own. 

One of the most common applications of unsupervised learning is in clustering, where the algorithm groups similar data points 
together based on their characteristics. For example, in the field of healthcare, unsupervised learning can be used to identify 
groups of patients who share similar symptoms, risk factors, or treatment responses, even when those groups are not explicitly 
labeled. This can help healthcare providers understand disease subtypes, personalize treatments, or design more targeted interventions.\cite{GoogleCloud-UL} 

\subsection{Reinforcement Learning}
Reinforcement learning is the third type of machine learning.
Unlike the previous branches of machine learning,
reinforcement learning doesnt rely on a dataset with predefined answers, it learns by experience.
In reinforcement learning, an agent learns to achieve a goal in
uncertain and potentially complex environments by receiving feedback
through rewards and penalties. \cite{RL-GeeksForGeeks}
\newline
The key concepts of reinforcement learning:
\begin{itemize}
      \item Agent: The learner/decision maker
            \begin{enumerate}
                  \item An agent is anything that can be viewed as perceiving its environment through
                        sensors and acting upon that environment. As mentioned above, the agent is the learner/decision maker because it
                        learns from its environment and makes decisions based on what it has learned.
                        For a software agent, the sensors could be the input data, network packets or keystrokes
                        and the actuators are the output data. \cite{IntelligentAgents}
            \end{enumerate}
      \item Environment: Everything the agent interacts with. The environment is the context in which the agent acts upon and learns from.
            There are a lot of different environment types, each described by the following attributes. \cite{IntelligentAgents}
            \begin{enumerate}
                  \item Deterministic vs Stochastic is the part of the environment that determines if the next state is completely
                        determined by the current state and agents action or if there is some randomness involved.
                  \item Episodic vs Sequential describes whether the agents experience is divided into unconnected episodes or
                        if the agents actions have long-term consequences on the environment.
                  \item Static vs Dynamic is whether the environment can change while the agent is making a decision.
                  \item Discrete vs Continuous is the number of possible actions and states the environment can provide the agent.
                  \item Single-Agent vs Multi-Agent is the number of agents operating in the same environment.
            \end{enumerate}
      \item State: A specific situation the agent finds itself in.
      \item Actions: What possible moves the agent can make.
      \item Rewards: Feedback from the environment, that helps guide the learning process.
      \item Policy: The agent's strategy for decision making.
      \item Value Function: Predicts future rewards of a state and/or action.
      \item Exploration vs Exploitation: Balancing between trying new actions or using known best actions.
\end{itemize}


\subsubsection{Types of Reinforcement Learning Agents}
Even in reinforcement learning there are subsets of algorithms that can be used. The two main types of reinforcement learning algorithms work in the following ways:
\begin{itemize}
      \item \textbf{Model-Free Reinforcement Learning} uses experience to learn directly from one or both of simpler quantities, state/action value-functions or policies. % TODO: I am not sure I understand what this says 
      \item \textbf{Model-Based Reinforcement Learning} uses experience to construct an internal model of the transitions and immediate outcomes in the environment.
\end{itemize}

\subsubsection{Model-free Reinforcement Learning}
Model-free methods bypass the need for an environmental model and instead
learns a policy or value function directly through interactions. These methods
can generally be simpler to implement but often, require a lot more training data 
and memory to store Q-tables for large environments.
Value-based methods, focus on the learning of a value function that estimates 
the expected return (cumulative reward) from a given state or state-action pair.
\begin{itemize}
      \item Q-learning is a value-based method, and is an off-policy algorithm,
      that learns the optimal action-value function \(Q*(s,a) \). The agent updates the 
      Q-values using to Bellman's equation: \cite{Types-of-Reinforcement-Learning}
      \begin{equation}
            Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
      \end{equation}
      \item SARSA or State-Action-Reward-State-Action is an on-policy method that 
      updates Q-values based on the current policy. \cite{SARSA}
      \begin{equation}
            Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma Q(s',a') - Q(s,a)]
      \end{equation}
      \item Deep Q-Networks (DQN) combines Q-learning with deep neural networks 
      to handle more complex environments with large state spaces. This approach leverages deep learning 
      to approximate Q-values. \cite{Types-of-Reinforcement-Learning}
\end{itemize}

Policy-based methods, on the other hand learn a policy directly without estimating
value functions. They are useful in continuous action spaces and stochastic policies.
\begin{itemize}
      \item \textbf{REINFORCE} is one such policy and is a Monte Carlo policy gradient method, 
      that optimizes the policy by adjusting the probability of taking actions that lead to a 
      higher cumulative reward. The policy is updated according to the gradient of expected rewards,
      where \(\mathbb{R}\) is the cumulative reward. \cite{Types-of-Reinforcement-Learning} % TODO: should this just be a normal R?
      \begin{equation}
            \nabla J(\theta) = \mathbb{E} \left[\nabla \log \pi_{\theta}(s_t,a_t) R_t \right]
            \label{equation:policy_gradient}
      \end{equation}
      \item \textbf{Actor-critic methods} combine both the policy learning (actor) with a value
      function estimator (critic) for a more stable learning process. \cite{Types-of-Reinforcement-Learning}
      \item \textbf{Proximal Policy Optimization (PPO)} is an improvement upon the basic policy
      gradient methods. It uses a clipped objective function to limit the amount of policy 
      changes that can be made, to prevent drastic changes during training, and thus improving stability. \cite{Types-of-Reinforcement-Learning}
\end{itemize}
Some disadvantages of policy-based methods is that they have a high variance in gradient estimates and often requires careful 
tuning of learning rates and other hyperparameters. \cite{Types-of-Reinforcement-Learning}

\subsubsection{Model-Based Reinforcement Learning}
Model-based RL aims to learn a model of the environment's transitions, dynamics % TODO: list? (comma separated or not)
and reward function. Once a model is accurate enough, it can be used to 
simulate trajectories and plan ahead, making an agent's learning significantly more
sample efficient. There are two main approaches to model-based RL \cite{ReinforcementLearning}:
\begin{itemize}
      \item Learn the model % TODO: flow?
      \item Given a model
\end{itemize}
Typically, model-based agents have a model of the environment, a planner function that uses
the model to search for optimal actions (e.g., via tree search or Optimization), and a policy
that may be learned directly or indirectly through planning.
Some types of model-based methods are:
\begin{itemize}
      \item Dyna Architecture (Sutton) that combines learning from real experiences, 
      with a simulated plan from the learned model.
      \item Monte Carlo Tree Seach (MCTS) which is used in conjunction with learned models
      to plan moves by simulating possible future states (Notably used in AlphaGo). \cite{ReinforcementLearning}
      \item Model predictive Control (MPC), is a planning based method, where the agent
      uses a learned or predefined model to predict a few steps ahead in the environment and 
      selects the action that optimizes the cumulative reward function over a planning horizon. \cite{Types-of-Reinforcement-Learning} 
\end{itemize}
The drawbacks of model-based methods is that although they can make more assumptions
and approximations on a given task, they are limited to only that specific 
type of task. It also requires an accurate model of the environment and building a model
can be computationally difficult and expensive and therefore lead to inaccuracies. \cite{Types-of-Reinforcement-Learning}

\subsection{Design Decision: Machine Learning Approach}
\label{sec:ML-Design-Decision}
In the designing of the machine learning model for this project, three core learning paradigms
were considered: Supervised learning, unsupervised learning and reinforcement learning. Each 
approach offers distinct advantages and challenges, but given the problem domain and 
project goals, reinforcement learning was ultimately selected as the most appropriate method.

\textbf{Supervised learning} is a powerful approach that is known for its high accuracy when trained on
sufficiently large and labeled datasets. However, it is not well-suited to this project for
two main reasons. Firstly, acquiring a large and detailed labeled dataset for Pokemon battles
would be impractical and time-consuming. Secondly, supervised models are static in nature, they learn
from pre-exisiting data and do not adapt during deployment. Since this project aims to develop
an autonomous agent, that interacts with and improves through the ever evolving environment of Pokemon battles, 
these limitations made supervised learning an unsuitable choice.

\textbf{Unsupervised learning}, while advantageous in situations where data is unlabeled, is primarily
geared towards pattern discovery and clustering. As such, it lacks the ability to learn 
optimal behavior through interaction and feedback. Given the goal of this projects agent is not
merely to identify patterns, but actively engage in all kinds of battles and make sequential decisions,
unsupervised learning was deemed unsuitable.

\textbf{Reinforcement learning}, by contrast offers a framework where an agent learns through direct interaction
with an enviromnent by performing actions and receiving feedback in the form of rewards or penalties.
This aligns closely with the nature of the normal Pokemon battle system, which is sequential, 
turn-based and governed by strategic decision making. Reinforcement learning allows the agent to
adapt and improve over time through trial and error, making it a well-suited choice for this project.

Having established reinforcement learning as the preferred approach, a specific algorithm
was selected to implement the agent's learning mechanism. The algorithm chosen is the 
Deep Q-Network (DQN). DQN was selected for a couple of reasons. Firstly, it supports discrete 
action spaces, which aligns well with Pokemon battles, where actions (moves, switches ect.) are
clearly defined and finite. Second, DQN is capable of learning from raw numerical representations
of state data, allowing for efficient learning from the complex state space of a Pokemon battle.
Finally, the use of experience replay and a target network, which are two core components of DQN, 
helps improve training stability and sample efficiency, which is crucial given the computational constraints
and training duration allotted for this project.
