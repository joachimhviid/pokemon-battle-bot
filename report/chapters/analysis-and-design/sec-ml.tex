\section{Machine Learning}
\label{sec:Machine Learning}

Machine learning is a subset of artificial intelligence
that focuses on the development of a computer program that learns from data.
In machine learning there are three approaches: supervised, unsupervised, and reinforcement learning.
Each of these approaches has its own strengths and weaknesses, but all of them need a machine learning algorithm.

A machine learning algorithm is a set of rules or mathematical
procedures that the system follows to learn from a dataset
and make predictions or decisions based on the data.

After applying the algorithm a dataset, you will have what is called a Machine learning Model.
A model is a specific instance of a trained system that has learned from the data and can make predictions or classifications.
\cite{ML-Models}
\textbf{Analogy:} A machine learning model is like a fully cooked meal.
The recipe (algorithm) was used to prepare it, but now it is a ready-to-use product.

\subsection{Supervised Learning}
The supervised learning approach is the more common approoach of the 3 approaches.
This is due to its ability to predict a wide range om problems accuratly,
however its effectiveness is dependent on the quality of the training data.
Supervised learning uses labled dataset to train the model,
which means that from the input data we expect the correct output data as well.
\cite{GoogleCloud-SL}

Some examples of supervised learning are spam email classifiers,
so wheter an email is spam or not and weather prediction models.

\subsection{Unsupervised Learning}
Unsupervised learning is a type of machine learning
that learns from data without human supervision. Unlike supervised learning,
unsupervised machine learning models are given unlabled data and allowed to discover patterns
and relationships without any explicit guidance or instructions. \cite{GoogleCloud-UL}

\subsection{Reinforcement Learning}
Reinforcement learning is the third type of machine learning.
Unlike the previous branches of machine learning,
reinforcement learning relies on a datasets with predefined answers, it learns by experience.
In reinforcement learning, an agent learns to achieve a goal in
an uncertain and potentially complex evironment/s py recieveing feedback
through rewards or penalties. \cite{RL-GeeksForGeeks}
\newline
The key concepts of reinforcement learning:
\begin{itemize}
      \item Agent: The learner/decision maker
            \begin{enumerate}
                  \item An agent is anything that can be viewed as percieving its environment through
                        sensors and acting upon that environment and as mentioned above, the agent is the learner/decision maker because it
                        learns from its environment and makes decisions based on what it has learned.
                        For a software agent the sensors could be the input data, network packes or keystrokes
                        and the actuators are the output data. \cite{IntelligentAgents}
            \end{enumerate}
      \item Environment: Everything the agent interacts with. The environment is the context in which the agent acts opun and learns from,
            and there exists a lot of different environment types. \cite{IntelligentAgents}
            \begin{enumerate}
                  \item Deterministic vs Stochastic is the part of the environment that determines if the next state is completly
                        determined by the current state and agents action or if there is some randomness involved.
                  \item Episodic vs Sequential describes wheter the agents experience is divided into unconnected episodes or
                        if the agents actions have long-term consequences on the environment.
                  \item Static vs Dynamic is wether the environment can change while the agent is making a decision.
                  \item Discrete vs Continuous is the number of possible actions and states the environment can provide the agent.
                  \item Single-Agent vs Multi-Agent is the number of agents operating in the same environment.
            \end{enumerate}
      \item State: A specific situation the agent finds itself in.
      \item Actions: What possible moves the agent can make.
      \item Rewards: Feedback from the environment, that helps guide the learning process.
      \item Policy: The agent's strategy for decision making.
      \item Value Function: Predicts future rewards of a state and/or action.
      \item Exploration vs Exploitation: Balancing between trying new action or using known best actions.
\end{itemize}


\subsubsection{Types of Reinforcement Learning Agents}
And even in reinfocement learning there exists subsets of algorithms that can be used, where
the two main types of reinforcement learning algorithms:
\begin{itemize}
      \item Model-Free Reinforcement Learning: Uses experience to learn directly from one or both of simpler quantities, state/action value-functions or policies.
      \item Model-Based Reinforcement Learning: Uses experience to construct an internal model of the 
      transitions and immediate outcomes in the environment.
\end{itemize}

\paragraph{Model-free Reinforcement Learning}
Model-free methods bypass the need for an environmental model and instead
learns a policy or value function directly through interactions. These metehods
can generally be simpler to implement but often, require a lot more training data 
and memory to store Q-tables for large environments.
Value-based methods, focus on the learning of a value function that estimates 
the expected return (cumulative reward) from a given state or state-action pair.
\begin{itemize}
      \item Q-learning is on of these value-based methods, and is an off policy algorithm,
      that learns the optimal action-value function \(Q*(s,a) \). The agent updates the 
      Q-values using to Bellman's equation: \cite{Types-of-Reinforcement-Learning}
      \begin{equation}
            Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
      \end{equation}
      \item SARSA or State-Action-Reward-State-Action is an on-policy method that 
      updates Q-values based on the current policy. \cite{SARSA}
      \begin{equation}
            Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma Q(s',a') - Q(s,a)]
      \end{equation}
      \item Deep Q-Networks (DQN) combines Q-learning with deep neural networks 
      to handle more complex environments with large state spaces. This approach leverages deep learning 
      to approximate Q-values. \cite{Types-of-Reinforcement-Learning}
\end{itemize}

Policy-based methods, on the other hand learn a policy direcly without estimating
value functions. They are usefull in continuous action spaces and stochastic policies.
\begin{itemize}
      \item REINFORCE is on one such policy and is a Monte Carlo policy gradient methods, 
      that optimizes the policy by adjusting probability of taking actions that lead to a 
      higher cumulative reward. The policy is updated accoding to the gradient of expected rewards,
      where \(\mathbb{R}\) is the cumulative reward. \cite{Types-of-Reinforcement-Learning}
      \begin{equation}
            \nabla J(\theta) = \mathbb{E} \left[\nabla \log \pi_{\theta}(s_t,a_t) R_t \right]
            \label{equation:policy_gradient}
      \end{equation}
      \item Actor-critic methods, combine both the policy learning (actor) with a value
      function estimator (critic) for a more stable learning process. \cite{Types-of-Reinforcement-Learning}
      \item Proximal Policy Optimization (PPO) is an improvement upon the basic policy
      gradient methods. It uses a clipped objective function to limit the amount of policy 
      changes, to prevent drastic cnages during training, and thus improving stability. \cite{Types-of-Reinforcement-Learning}
\end{itemize}
Some disadvantages of policy-based methods is they have a high variance in gradient estimates and often requires careful 
tuning of learning rates and other hyperparameters. \cite{Types-of-Reinforcement-Learning}

\paragraph{Model-Based Reinforcement Learning}
Model-based RL aims to learn a model of the environment's transitions dynamics
and reward function. Once a model is accurate enough, it can be used to 
simulate trajectories and plan ahead, making an agent learning significantly more
sample efficient. There are two main approaches to model-based RL \cite{ReinforcementLearning}:
\begin{itemize}
      \item Learn the model 
      \item Given a model
\end{itemize}
Typically, model-based agents has a model of the environment, a planner function that uses
the model to search for optimal actions (e.g., via tree search or Optimization), and a policy
that may be learned directly or indirectly through planning.
Some types of model-based methods are:
\begin{itemize}
      \item Dyna Architecture (Sutton) that combines learning from lear experiences, 
      with a simulated planning from the learned model.
      \item Monte Carlo Tree Seach (MCTS) which is used in conjunction with learned models
      to plan moves by simulating possible future states (Notably used in AlphaGo). \cite{ReinforcementLearning}
      \item Model predictive Control (MPC), is a planning based method, where the agent
      uses a learned or predefined model to predict a few stepts in the environment and 
      selcts the action that optiomizes the cumulative reward function over a planning horizon. \cite{Types-of-Reinforcement-Learning} 
\end{itemize}
The drawbacks of model-based methods is that altough they can have more assumptions
and approximations on a given task, but they may only be limited to that sepcific 
type of task. It also requires accurate models of the environment and building a model
can be computationally difficult and expensive and therefore lead to inaccuracies. \cite{Types-of-Reinforcement-Learning}

\subsection{Choice of Machine learning model}

From these pros and cons of all three machine learning approaches, we can see that supervised learning is the most accurate and widely used approach,
but for this project, we will be using reinforcement learning going forward.

\begin{table}[H]      
      \begin{tabular}{| m{3.5cm} | m{5cm} | m{5cm} |}
            \hline
            \textbf{Type} & \textbf{Pros} & \textbf{Cons} \\ 
            \hline
            \textbf{Supervised Learning} & 
            \begin{itemize}
                  \item Highly accurate when trained on quality data.
                  \item Well-understood and widely used.
                  \item Can be used for both classification and regression tasks.
            \end{itemize} & 
            \begin{itemize}
                  \item Requires large labeled datasets, which can be expensive.
                  \item Cannot handle unseen data well without retraining.
                  \item Prone to overfitting if the dataset is too small.
            \end{itemize} \\ 
            \hline
            \textbf{Unsupervised Learning} & 
            \begin{itemize}
                  \item Can discover hidden patterns in data.
                  \item No need for labeled data, reducing data preparation costs.
                  \item Useful for clustering and anomaly detection.
            \end{itemize} & 
            \begin{itemize}
                  \item Hard to evaluate results objectively.
                  \item May identify patterns that are not meaningful.
                  \item Requires significant computational power for large datasets.
            \end{itemize} \\ 
            \hline
            \textbf{Reinforcement Learning} & 
            \begin{itemize}
                  \item Learns optimal behavior through trial and error.
                  \item Can adapt to dynamic environments.
                  \item Suitable for decision-making problems like robotics and game playing.
            \end{itemize} & 
            \begin{itemize}
                  \item Training can be very slow and resource-intensive.
                  \item Requires a well-designed reward system to work effectively.
                  \item Can struggle with long-term dependencies if not tuned properly.
            \end{itemize} \\ 
            \hline
      \end{tabular}
      \caption{Comparison of Supervised, Unsupervised, and Reinforcement Learning}
      \label{tab:ml_comparison}
\end{table}
